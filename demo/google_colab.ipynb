{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyGk-87qnbWE"
   },
   "source": [
    "# bia-bob demo in Colab\n",
    "\n",
    "In this notebook we willl use [bia-bob](https://github.com/haesleinhuepf/bia-bob) for AI-based generation of bioimage analysis (BIA) code. While bia-bob is per default using commercial large language models (LLMs), it can make use of locally running, privacy and cost preserving LLMs using [Ollama](https://ollama.com/). This notebook is inspired by / based on code in [collama](https://github.com/5aharsh/collama). By default, Google Colab sessions run on a CPU. To speed up LLM model execution, you can switch to a T4 GPU (or a more powerful option if you have a paid Colab plan). To do this, navigate to **Runtime > Change runtime type** and select the desired GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1S1YL6EnYBB"
   },
   "source": [
    "## Setup\n",
    "\n",
    "### Install necessary packages\n",
    "\n",
    "1. `pciutils` helps Ollama identify available GPU hardware.\n",
    "2. Ollama is installed in the runtime using `curl -fsSL https://ollama.com/install.sh | sh`.\n",
    "3. The `bia-bob` package is also installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YlVK9iG4AD5L",
    "outputId": "4e231fca-ec0e-4cc2-e42f-f1ac13346a77"
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "!pip install --no-cache-dir --quiet \"bia-bob>=0.34.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download some test data\n",
    "Here we just download a small image for testing. Consider reading the [Google Colab guide for how to access data](https://colab.research.google.com/notebooks/io.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/haesleinhuepf/bia-bob/raw/refs/heads/main/demo/blobs.tif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcBLqZfyoHg4"
   },
   "source": [
    "### Local privacy preserving Ollama Models\n",
    "\n",
    "Here we will use the model [Gemma3:4b](https://ollama.com/library/gemma3:4b) a small language model with basic capabilities. For other Ollama models check https://ollama.com/library . The advantages of such a locally running model are a) we do not have to pay API usage and b) our prompts and data are not sent to a thrid-party.\n",
    "\n",
    "If the model is used the first time, this may take a moment as the model needs to be downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try bia-bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hb49kH5BuiF1"
   },
   "outputs": [],
   "source": [
    "from bia_bob import bob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNLpYHpEuwjb",
    "outputId": "5ed682e1-b2ec-47da-fe52-57754722f4ce"
   },
   "outputs": [],
   "source": [
    "bob.initialize(endpoint='ollama', model='gemma3:4b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ETlkP2Nu-Bl",
    "outputId": "a1a1fa6c-421e-483d-ff94-4bfb22435638"
   },
   "outputs": [],
   "source": [
    "%bob load blobs.tif and show it"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
